<div align="center">
    <h1>
        Chinese-Mixtral-8x7B
    </h1>
</div>

![](img/logo.png)

<div align="center">
    <a href="https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B/pulls">
        <image src="https://img.shields.io/badge/PRs-welcome-brightgreen"></image>
        <image src="https://img.shields.io/badge/License-Apache_2.0-green.svg"></image>
    </a>
</div>

## ğŸ‰ æ–°é—»

- [2024-02-09] å‘å¸ƒåŸºäºChinese-Mixtral-8x7BæŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹ï¼š[æ´»å­— 3.0](https://github.com/HIT-SCIR/huozi)ï¼›å¼€æºæŒ‡ä»¤å¾®è°ƒä»£ç ã€‚
- [2024-01-18] å‘å¸ƒChinese-Mixtral-8x7BåŸºåº§æ¨¡å‹ï¼›å¼€æºå¢é‡é¢„è®­ç»ƒä»£ç ã€‚

## ğŸš€ ä»‹ç»

æœ¬é¡¹ç›®åŸºäºMistralå‘å¸ƒçš„æ¨¡å‹[Mixtral-8x7B](https://mistral.ai/news/mixtral-of-experts/)è¿›è¡Œäº†ä¸­æ–‡æ‰©è¯è¡¨å¢é‡é¢„è®­ç»ƒï¼Œå¸Œæœ›è¿›ä¸€æ­¥ä¿ƒè¿›ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç¤¾åŒºå¯¹MoEæ¨¡å‹çš„ç ”ç©¶ã€‚æˆ‘ä»¬æ‰©å……åçš„è¯è¡¨æ˜¾è‘—æé«˜äº†æ¨¡å‹å¯¹ä¸­æ–‡çš„ç¼–è§£ç æ•ˆç‡ï¼Œå¹¶é€šè¿‡å¤§è§„æ¨¡å¼€æºè¯­æ–™å¯¹æ‰©è¯è¡¨æ¨¡å‹è¿›è¡Œå¢é‡é¢„è®­ç»ƒï¼Œä½¿æ¨¡å‹å…·å¤‡äº†å¼ºå¤§çš„ä¸­æ–‡ç”Ÿæˆå’Œç†è§£èƒ½åŠ›ã€‚

é¡¹ç›®å¼€æºå†…å®¹ï¼š

- ä¸­æ–‡Mixtral-8x7Bæ‰©è¯è¡¨å¤§æ¨¡å‹
- æ‰©è¯è¡¨å¢é‡é¢„è®­ç»ƒä»£ç 

> è¯·æ³¨æ„ï¼ŒChinese-Mixtral-8x7Bä»ç„¶å¯èƒ½ç”ŸæˆåŒ…å«äº‹å®æ€§é”™è¯¯çš„è¯¯å¯¼æ€§å›å¤æˆ–åŒ…å«åè§/æ­§è§†çš„æœ‰å®³å†…å®¹ï¼Œè¯·è°¨æ…é‰´åˆ«å’Œä½¿ç”¨ç”Ÿæˆçš„å†…å®¹ï¼Œè¯·å‹¿å°†ç”Ÿæˆçš„æœ‰å®³å†…å®¹ä¼ æ’­è‡³äº’è”ç½‘ã€‚

## ğŸ“¥ æ¨¡å‹ä¸‹è½½

æœ¬é¡¹ç›®ä½¿ç”¨QLoRAè¿›è¡Œè®­ç»ƒï¼ŒLoRAæƒé‡ä¸åˆå¹¶æƒé‡åçš„æ¨¡å‹åˆ†åˆ«å¼€æºï¼Œæ‚¨å¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€æ±‚é€‰æ‹©ä¸‹è½½ï¼š

|             æ¨¡å‹åç§°             | æ¨¡å‹å¤§å°  |                                     ä¸‹è½½åœ°å€                                      |                                                         å¤‡æ³¨                                                          |
|:----------------------------:|:-----:|:-----------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------------------------------------:|
|     Chinese-Mixtral-8x7B     | 88GB  |     [HuggingFace](https://huggingface.co/HIT-SCIR/Chinese-Mixtral-8x7B)<br>[ModelScope](https://modelscope.cn/models/HIT-SCIR/Chinese-Mixtral-8x7B/summary)     |                                                  ä¸­æ–‡æ‰©è¯è¡¨å®Œæ•´æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨                                                   |
| Chinese-Mixtral-8x7B-adapter | 2.7GB | [HuggingFace](https://huggingface.co/HIT-SCIR/Chinese-Mixtral-8x7B-adapter) | LoRAæƒé‡ï¼Œéœ€è¦ä¸åŸç‰ˆMixtral-8x7Bè¿›è¡Œåˆå¹¶æ‰å¯ä»¥ä½¿ç”¨ï¼Œåˆå¹¶è„šæœ¬è¯·å‚è€ƒ[è¿™é‡Œ](https://gist.github.com/ChrisHayduk/1a53463331f52dca205e55982baf9930) |

## ğŸ’» æ¨¡å‹æ¨ç†

Chinese-Mixtral-8x7Bæ”¯æŒå®Œæ•´çš„Mixtral-8x7Bæ¨¡å‹ç”Ÿæ€ï¼ŒåŒ…æ‹¬ä½¿ç”¨`vLLM`ã€`Flash Attention 2`è¿›è¡ŒåŠ é€Ÿï¼Œä½¿ç”¨`bitsandbytes`è¿›è¡Œæ¨¡å‹é‡åŒ–ç­‰ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨Chinese-Mixtral-8x7Bè¿›è¡Œæ¨ç†çš„ä»£ç ç¤ºä¾‹ã€‚

ä½¿ç”¨Flash Attention 2ï¼š
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "HIT-SCIR/Chinese-Mixtral-8x7B"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation="flash_attention_2", torch_dtype=torch.bfloat16, device_map="auto")

text = "æˆ‘çš„åå­—æ˜¯"
inputs = tokenizer(text, return_tensors="pt").to(0)

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

ä½¿ç”¨4bité‡åŒ–ï¼š
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "HIT-SCIR/Chinese-Mixtral-8x7B"
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map="auto")

text = "æˆ‘çš„åå­—æ˜¯"
inputs = tokenizer(text, return_tensors="pt").to(0)

outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

è¯·æ³¨æ„ï¼ŒChinese-Mixtral-8x7Bä¸ºåŸºåº§æ¨¡å‹ï¼Œæ²¡æœ‰ç»è¿‡æŒ‡ä»¤å¾®è°ƒï¼Œå› æ­¤æŒ‡ä»¤éµå¾ªèƒ½åŠ›æœ‰é™ã€‚æ‚¨å¯ä»¥å‚è€ƒ[å¾®è°ƒ](#å¾®è°ƒ)ä¸€èŠ‚å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

## ğŸ“ˆ æ¨¡å‹æ€§èƒ½

### æ¨¡å‹ç»¼åˆèƒ½åŠ›

æˆ‘ä»¬åˆ†åˆ«ä½¿ç”¨ä»¥ä¸‹è¯„æµ‹æ•°æ®é›†å¯¹Chinese-Mixtral-8x7Bè¿›è¡Œè¯„æµ‹ï¼š

- C-Evalï¼šä¸€ä¸ªå…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ã€‚å®ƒåŒ…å«äº†13948ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œæ¶µç›–äº†52ä¸ªä¸åŒçš„å­¦ç§‘å’Œå››ä¸ªéš¾åº¦çº§åˆ«ã€‚
- CMMLUï¼šä¸€ä¸ªç»¼åˆæ€§çš„ä¸­æ–‡è¯„ä¼°åŸºå‡†ï¼Œä¸“é—¨ç”¨äºè¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨ä¸­æ–‡è¯­å¢ƒä¸‹çš„çŸ¥è¯†å’Œæ¨ç†èƒ½åŠ›ï¼Œæ¶µç›–äº†ä»åŸºç¡€å­¦ç§‘åˆ°é«˜çº§ä¸“ä¸šæ°´å¹³çš„67ä¸ªä¸»é¢˜ã€‚
- MMLUï¼šä¸€ä¸ªåŒ…å«57ä¸ªå¤šé€‰ä»»åŠ¡çš„è‹±æ–‡è¯„æµ‹æ•°æ®é›†ï¼Œæ¶µç›–äº†åˆç­‰æ•°å­¦ã€ç¾å›½å†å²ã€è®¡ç®—æœºç§‘å­¦ã€æ³•å¾‹ç­‰ï¼Œéš¾åº¦è¦†ç›–é«˜ä¸­æ°´å¹³åˆ°ä¸“å®¶æ°´å¹³ï¼Œæ˜¯ç›®å‰ä¸»æµçš„LLMè¯„æµ‹æ•°æ®é›†ä¹‹ä¸€ã€‚
- HellaSwagï¼šä¸€ä¸ªæå…·æŒ‘æˆ˜çš„è‹±æ–‡NLIè¯„æµ‹æ•°æ®é›†ï¼Œæ¯ä¸€ä¸ªé—®é¢˜éƒ½éœ€è¦å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œæ·±å…¥ç†è§£ï¼Œè€Œä¸èƒ½åŸºäºå¸¸è¯†è¿›è¡Œå›ç­”ã€‚

æ ¹æ®Mistralå‘å¸ƒçš„[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/pdf/2401.04088.pdf)ï¼ŒMixtral-8x7Båœ¨æ¨ç†æ—¶å°†æ¿€æ´»13Bå‚æ•°ã€‚ä¸‹è¡¨ä¸ºChinese-Mixtral-8x7Bä¸å…¶ä»–13Bè§„æ¨¡çš„ä¸­æ–‡æ‰©è¯è¡¨æ¨¡å‹åœ¨å„ä¸ªè¯„æµ‹æ•°æ®é›†ä¸Šçš„5-shotç»“æœï¼š

|                                              æ¨¡å‹åç§°                                               |      å¢é‡è®­ç»ƒè¯­æ–™       | C-Eval<br>(ä¸­æ–‡) | CMMLU<br>(ä¸­æ–‡) | MMLU<br>(è‹±æ–‡) | HellaSwag<br>(è‹±æ–‡) |
|:-----------------------------------------------------------------------------------------------:|:-----------------:|:--------------:|:-------------:|:------------:|:-----------------:|
|           [IDEA-CCNL/Ziya2-13B-Base](https://huggingface.co/IDEA-CCNL/Ziya2-13B-Base)           |    650B Token     |     59.29      |     60.93     |    59.86     |       58.90       |
| [TigerResearch/tigerbot-13b-base-v3](https://huggingface.co/TigerResearch/tigerbot-13b-base-v3) |    500B Token     |     50.52      |     51.65     |    53.46     |       59.16       |
|    [Linly-AI/Chinese-LLaMA-2-13B-hf](https://huggingface.co/Linly-AI/Chinese-LLaMA-2-13B-hf)    |     11B Token     |     42.57      |     41.95     |    51.32     |       59.05       |
|            [hfl/chinese-llama-2-13b](https://huggingface.co/hfl/chinese-llama-2-13b)            | çº¦30B Token(120GB) |     41.90      |     42.08     |    51.92     |       59.28       |
|                                  **Chinese-Mixtral-8x7B(æœ¬é¡¹ç›®)**                                  |     42B Token     |     52.08      |     51.08     |    69.80     |       65.69       |

åœ¨ä¸­æ–‡çŸ¥è¯†å’Œç†è§£æ–¹é¢ï¼Œæˆ‘ä»¬çš„Chinese-Mixtral-8x7Bä¸TigerBot-13B-Base-v3æ€§èƒ½ç›¸å½“ã€‚ç”±äºChinese-Mixtral-8x7Bçš„è®­ç»ƒæ•°æ®é‡ä»…ä¸ºTigerBot-13B-Base-v3çš„8%ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä»æœ‰è¿›ä¸€æ­¥æå‡çš„ç©ºé—´ã€‚ä¸æ­¤åŒæ—¶ï¼Œå¾—ç›ŠäºåŸç‰ˆMixtral-8x7Bæ¨¡å‹å¼ºå¤§çš„æ€§èƒ½ï¼Œæˆ‘ä»¬çš„Chinese-Mixtral-8x7Bè¾¾åˆ°äº†å„ä¸ªæ‰©è¯è¡¨æ¨¡å‹çš„æœ€å¼ºè‹±æ–‡æ°´å¹³ã€‚

> ç”±äºä¸åŒç‰ˆæœ¬çš„è¯„æµ‹è„šæœ¬å®ç°ç»†èŠ‚æœ‰ç»†å¾®å·®å¼‚ï¼Œä¸ºäº†ä¿è¯è¯„æµ‹ç»“æœçš„ä¸€è‡´æ€§å’Œå…¬å¹³æ€§ï¼Œæˆ‘ä»¬çš„è¯„æµ‹è„šæœ¬ç»Ÿä¸€ä½¿ç”¨EleutherAIå‘å¸ƒçš„lm-evaluation-harnessï¼Œcommit hashä¸º[28ec7fa](https://github.com/EleutherAI/lm-evaluation-harness/tree/28ec7fa950346b5a895e85e1f3edd5648168acc4)ã€‚

### æ¨¡å‹ç”Ÿæˆæ•ˆæœ

ä¸‹è¡¨ä¸ºå„ä¸ªæ‰©è¯è¡¨æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœã€‚ç”±äºéƒ¨åˆ†æ¨¡å‹çš„é¢„è®­ç»ƒè¯­æ–™æœªä½¿ç”¨`eos_token`è¿›è¡Œåˆ†éš”ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†`max_tokens = 100`å¯¹ç”Ÿæˆæ–‡æœ¬è¿›è¡Œæˆªæ–­ã€‚æˆ‘ä»¬çš„é‡‡æ ·å‚æ•°ä¸º`temperature = 0.8, top_p = 0.9`ã€‚

![](./img/case.png)

### ä¸­æ–‡ç¼–è§£ç æ•ˆç‡

é’ˆå¯¹ä¸­æ–‡ç¼–è§£ç æ•ˆç‡ï¼Œæˆ‘ä»¬ä½¿ç”¨å„ä¸ªæ‰©è¯è¡¨æ¨¡å‹çš„åˆ†è¯å™¨å¯¹[SkyPile](https://huggingface.co/datasets/Skywork/SkyPile-150B)æ•°æ®é›†çš„ä¸€ä¸ªåˆ‡ç‰‡ï¼ˆ2023-06_zh_head_0000.jsonlï¼‰è¿›è¡Œç¼–ç ï¼Œå¯¹æ¯”äº†å„ä¸ªåˆ†è¯å™¨è¾“å‡ºçš„ä¸­æ–‡æ–‡æœ¬Tokené‡ï¼š

|                æ¨¡å‹åç§°                |  æ¨¡å‹ç±»åˆ«   | è¯è¡¨å¤§å°  | ä¸­æ–‡æ–‡æœ¬Tokené‡ | ç¼–è§£ç æ•ˆç‡ |
|:----------------------------------:|:-------:|:-----:|:----------:|:-------:|
|     meta-llama/Llama-2-13B-hf      |  LLaMA  | 32000 |    780M    |    ä½    |
|    mistralai/Mixtral-8x7B-v0.1     | Mixtral | 32000 |    606M    |    ä½    |
|  Linly-AI/Chinese-LLaMA-2-13B-hf   |  LLaMA  | 40076 |    532M    |  ä¸­  |
|      IDEA-CCNL/Ziya2-13B-Base      |  LLaMA  | 39424 |    532M    |  ä¸­  |
|      hfl/chinese-llama-2-13b       |  LLaMA  | 55296 |    365M    |  é«˜  |ã€
| TigerResearch/tigerbot-13b-base-v3 |  LLaMA  | 65112 |    342M    |  é«˜  |
|   **Chinese-Mixtral-8x7B(æœ¬é¡¹ç›®)**    | Mixtral | 57000 |    355M    |  é«˜  |

åœ¨çº¦1.4GBçš„æµ‹è¯•æ–‡æœ¬ä¸­ï¼Œæˆ‘ä»¬çš„Chinese-Mixtral-8x7Bä¸­æ–‡ç¼–è§£ç æ•ˆç‡ä»…æ¬¡äºTigerBot-13B-Base-v3ï¼Œè¾ƒåŸæ¨¡å‹æé«˜äº†41.5%ã€‚è¿™æœ‰åˆ©äºåŠ é€Ÿä¸­æ–‡æ–‡æœ¬çš„æ¨ç†é€Ÿåº¦ï¼Œå¹¶åœ¨In-Context Learningã€Chain-of-Thoughtç­‰åœºæ™¯ä¸­èŠ‚çœåºåˆ—é•¿åº¦ï¼Œæœ‰åˆ©äºæé«˜å¤æ‚æ¨ç†ä»»åŠ¡çš„æ€§èƒ½ã€‚

## âš™ï¸ è®­ç»ƒç»†èŠ‚

<details>
<summary>

### è¯è¡¨æ‰©å……

</summary>

æˆ‘ä»¬ä½¿ç”¨`sentencepiece`åœ¨12GçŸ¥ä¹æ•°æ®å’Œ2Gæ‚Ÿé“æ•°æ®ä¸Šè®­ç»ƒä¸­æ–‡BPEè¯è¡¨ã€‚æˆ‘ä»¬åœ¨è®­ç»ƒè¯è¡¨æ—¶åˆ†åˆ«æšä¸¾äº†ä¸­æ–‡å•å­—Tokenæ•°é‡ä»¥åŠä¸­æ–‡æ€»Tokenæ•°é‡ï¼Œå¹¶å¯¹äºŒè€…è¿›è¡Œç»„åˆï¼Œå¾—åˆ°äº†æ•°ç™¾ä¸ªå¤§å°ã€å†…å®¹å„å¼‚çš„è¯è¡¨ã€‚ä¸ºäº†å¾—åˆ°æœ€é€‚åˆçš„è¯è¡¨ï¼Œæˆ‘ä»¬é€šè¿‡Zheng Boç­‰äººæå‡ºçš„[ALP](https://arxiv.org/pdf/2109.07306.pdf)è®¡ç®—è¿™äº›è¯è¡¨çš„ä¸­æ–‡è¯æ±‡èƒ½åŠ›ã€‚ALPé€šè¿‡è®¡ç®—ç‰¹å®šè¯­è¨€çš„å­è¯åˆ‡åˆ†ç²’åº¦ï¼Œå¹¶å¯¹è¯è¡¨çš„ä¸­ä½é¢‘å­è¯è¿›è¡Œæƒ©ç½šï¼Œæ˜¯ä¸€ç§æ–¹ä¾¿å¿«æ·çš„è¡¡é‡ç‰¹å®šè¯­è¨€è¯æ±‡èƒ½åŠ›çš„æŒ‡æ ‡ã€‚

æˆ‘ä»¬åœ¨ä¹¦ç±å’Œç™¾ç§‘è¯­æ–™ä¸Šè¯„ä¼°äº†ä¸åŒè¯è¡¨çš„ALPå€¼ã€‚å›¾ç¤ºä¸­ï¼Œå››æ¡æ›²çº¿åˆ†åˆ«ä»£è¡¨å››ç§ä¸­æ–‡å•å­—Tokenæ•°é‡çš„è¯è¡¨ï¼ˆ4451ã€5435ã€6414å’Œ7434ï¼‰ã€‚ä¸ºäº†é¿å…è¯è¡¨è¿‡å°å¯¼è‡´ä¸­æ–‡å‹ç¼©ç‡è¿‡ä½ï¼Œä»¥åŠè¯è¡¨è¿‡å¤§å¯¼è‡´embeddingå±‚è¿‡äºç¨€ç–ï¼Œæˆ‘ä»¬é€‰å–ALPæ›²çº¿çš„æ‹ç‚¹ï¼Œå¯¹åº”å‘è¯è¡¨ä¸­æ–°å¢25000ä¸ªä¸­æ–‡Tokenã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬é€‰æ‹©äº†å››æ¡æ›²çº¿ä¸­ALPæœ€å¤§è€…ï¼Œå³æ–°å¢6414ä¸ªä¸­æ–‡å•å­—Tokençš„è¯è¡¨ï¼Œä½œä¸ºæœ€ç»ˆChinese-Mixtral-8x7Bé€‰ç”¨çš„è¯è¡¨ã€‚

![](./img/alp.png)

åœ¨è·å¾—æ–°è¯è¡¨åï¼Œæˆ‘ä»¬éœ€è¦å¯¹embeddingå’Œlm_headå±‚è¿›è¡Œæ‰©å……å’Œåˆå§‹åŒ–ã€‚æˆ‘ä»¬ä½¿ç”¨æ–°Tokenåœ¨æ—§embeddingå±‚ä¸­çš„è¯åµŒå…¥å¹³å‡å€¼å¯¹æ‰©å……éƒ¨åˆ†è¿›è¡Œåˆå§‹åŒ–ã€‚åœ¨æˆ‘ä»¬çš„å‰æœŸå®éªŒä¸­ï¼Œè¿™ç§æ–¹æ³•ç•¥ä¼˜äºHuggingFaceçš„é»˜è®¤å®ç°ï¼Œå³ä½¿ç”¨å›ºå®šçš„æ­£æ€åˆ†å¸ƒè¿›è¡Œåˆå§‹åŒ–ã€‚

</details>

<details>
<summary>

### å¢é‡é¢„è®­ç»ƒ

</summary>

Mixtral-8x7Bæ¨¡å‹å‚æ•°é‡ä¸º46.7Bï¼Œå…¨å‚æ•°è®­ç»ƒéœ€è¦åŒæ—¶ä½¿ç”¨å¤šç§å¹¶è¡Œç­–ç•¥ï¼Œåœ¨è®­ç»ƒèµ„æºå—é™çš„æƒ…å†µä¸‹æ—¶é—´æˆæœ¬è¿‡é«˜ã€‚å› æ­¤æˆ‘ä»¬é‡‡ç”¨HuggingFaceå®˜æ–¹æ¨èçš„æ–¹æ³•ï¼Œä½¿ç”¨QLoRAå¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚QLoRAåœ¨LoRAä½ç§©åˆ†è§£çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡å¼•å…¥4ä½é‡åŒ–ã€åŒé‡é‡åŒ–å’Œåˆ©ç”¨NVIDIAç»Ÿä¸€å†…å­˜è¿›è¡Œåˆ†é¡µï¼Œè¿›ä¸€æ­¥å‡å°‘äº†è®­ç»ƒæ‰€éœ€æ˜¾å­˜ï¼ŒåŒæ—¶ä¿æŒäº†ä¸å…¨å‚æ•°è®­ç»ƒç›¸å½“çš„æ€§èƒ½ã€‚

æˆ‘ä»¬å‚è€ƒYiming Cuiç­‰äºº[å¯¹LoRAçš„è®¾ç½®](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/blob/main/scripts/training/run_pt.sh)ï¼Œå¯¹åŸæ¨¡å‹æ‰€æœ‰Linearå±‚åº”ç”¨ä½ç§©åˆ†è§£ï¼Œå¹¶å°†æ‰©å¢åçš„embeddingå’Œlm_headå±‚çš„å‚æ•°è®¾ç½®ä¸ºå¯è®­ç»ƒã€‚å¯¹äºæ¨¡å‹ä¸»ä½“ï¼Œæˆ‘ä»¬é‡‡ç”¨NF4æ ¼å¼è¿›è¡Œé‡åŒ–ï¼Œè¿™ç§æ ¼å¼å¯ä»¥ä½¿å¾—é‡åŒ–åçš„æ•°æ®ä¸é‡åŒ–å‰å…·æœ‰åŒç­‰çš„æ•°æ®åˆ†å¸ƒï¼Œæ¨¡å‹çš„æƒé‡ä¿¡æ¯æŸå¤±æ›´å°‘ã€‚

#### ç¯å¢ƒå‡†å¤‡

æˆ‘ä»¬å»ºè®®ä½¿ç”¨Python 3.10 + torch 2.0.1

```shell
# Pytorch + Transformers
$ pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2
$ pip install transformers==4.36.2 datasets evaluate peft accelerate gradio optimum sentencepiece trl
$ pip install jupyterlab scikit-learn pandas matplotlib tensorboard nltk rouge bitsandbytes fire
# CUDA Toolkit
$ conda install nvidia/label/cuda-11.7.1::cuda
# DeepSpeed
$ git clone https://github.com/microsoft/DeepSpeed.git
$ cd DeepSpeed
$ DS_BUILD_FUSED_ADAM=1 pip3 install .
# Flash Attention
$ pip install flash-attn --no-build-isolation
```

#### æ•°æ®é›†ä¸‹è½½

æˆ‘ä»¬åŸºäºç°æœ‰çš„å¼€æºæ•°æ®é›†è®­ç»ƒäº†Chinese-Mixtral-8x7Bï¼Œæ•°æ®é›†åŒ…æ‹¬ï¼š

|                                    æ•°æ®é›†åç§°                                     | æ•°æ®é›†è¯­è¨€ |ä½¿ç”¨æ•°æ®é‡|        å¤‡æ³¨        |
|:----------------------------------------------------------------------------:|:-----:|:----------------:|:-----:|
| [Skywork/SkyPile-150B](https://huggingface.co/datasets/Skywork/SkyPile-150B) |  ä¸­æ–‡   |30B| ä»…ä½¿ç”¨2022 + 2023å¹´çš„æ•°æ® |
| [DKYoon/SlimPajama-6B](https://huggingface.co/datasets/DKYoon/SlimPajama-6B) |  è‹±æ–‡   |12B|        æ•°æ®é›†é‡å¤2 Epoch         |

é€šè¿‡`data/download.py`å°†æ•°æ®é›†ä¸‹è½½åˆ°`data`ä¸­ã€‚é’ˆå¯¹Slimpajamaæ•°æ®é›†ï¼Œéœ€è¦ä½¿ç”¨`data/parquet2jsonl.py`å°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸º`jsonl`æ ¼å¼ã€‚

ä¸‹è½½åçš„æ•°æ®é›†ä¸ºå¤šä¸ªjsonlæ–‡ä»¶çš„åˆ†ç‰‡ï¼Œä½¿ç”¨`cat`å°†å¤šä¸ªåˆ†ç‰‡åˆå¹¶ä¸ºä¸€ä¸ªjsonlæ–‡ä»¶ã€‚

```shell
$ cat *.jsonl > all.jsonl
```

é€šè¿‡`split`å°†jsonlåˆ‡åˆ†ä¸ºtrainå’Œvalidé›†åˆã€‚æœ¬é¡¹ç›®ä¸­trainå’Œvalidçš„è¡Œæ•°æ¯”ä¾‹ä¸º999:1ã€‚

```shell
$ wc -l all.jsonl                          # è®¡ç®—æ•°æ®é›†æ€»è¡Œæ•°
$ split -l <lines> all.jsonl               # æŒ‰999:1è®¡ç®—train/validè¡Œæ•°ï¼Œè¿›è¡Œåˆ‡åˆ†
$ mv xaa DKYoon-SlimPajama-6B-train.jsonl  # é‡å‘½å
$ mv xab DKYoon-SlimPajama-6B-dev.jsonl
```

#### æ•°æ®é›†é¢„å¤„ç†

å°†æ•°æ®é›†åç§°å’Œè·¯å¾„æ³¨å†Œåˆ°`data/datasets.toml`ä¸­ï¼š

```toml
[DKYoon-SlimPajama-6B]              # æ•°æ®é›†åç§°
splits = ["train", "dev"]           # æ•°æ®é›†train/validé›†åˆ
root = "{DATA_DIR}/en/{name}"       # æ•°æ®é›†æ ¹ç›®å½•
doc = "{name}-{split}"              # æ•°æ®é›†æ–‡ä»¶å
encoded = "encoded-{name}-{split}"  # é¢„å¤„ç†ä¿å­˜ä½ç½®
```

ä½¿ç”¨`data/preprocess_datasets.py`å¯¹æ•°æ®é›†è¿›è¡Œå­è¯åˆ‡åˆ†ï¼Œä»è€ŒåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚

```shell
$ python data/preprocess_datasets.py --ds_name SkyPile-150B-2023 --tokenizer_name_or_path tokenizer/Mixtral-8x7B-v0.1-vocab
$ python data/preprocess_datasets.py --ds_name DKYoon-SlimPajama-6B --tokenizer_name_or_path tokenizer/Mixtral-8x7B-v0.1-vocab
```

åœ¨è¿›è¡Œå­è¯åˆ‡åˆ†åï¼Œå¯ä»¥ä½¿ç”¨`data/utils.py`æŸ¥çœ‹å„ä¸ªæ•°æ®é›†çš„tokenæ€»é‡ï¼š

```shell
$ python data/utils.py
```

#### å¼€å§‹è®­ç»ƒ

è®­ç»ƒå¯åŠ¨è„šæœ¬ä¸º`scripts/train.sh`ã€‚å¯ä»¥é€šè¿‡ä¿®æ”¹å…¶ä¸­çš„`TRAIN_DATASETS`ä¿®æ”¹è®­ç»ƒæ•°æ®é›†å’Œæ•°æ®é›†æ¯”ä¾‹ï¼š

```shell
TRAIN_DATASETS=(
    1:SkyPile-150B-2022     # ä½¿ç”¨å…¨é‡SkyPile-150B-2022
    0.1:SkyPile-150B-2023   # ä½¿ç”¨SkyPile-150B-2023çš„10%æ•°æ®
    1:DKYoon-SlimPajama-6B  # ä½¿ç”¨å…¨é‡DKYoon-SlimPajama-6B
)
```

å¦‚æœæ‚¨ä½¿ç”¨SLURMé›†ç¾¤ç®¡ç†ç³»ç»Ÿï¼Œå¯ä»¥é€šè¿‡`sbatch`è¿›è¡Œæäº¤ï¼š

```shell
$ sbatch scripts/train-pt.sh
```

å¦‚æœæ²¡æœ‰SLURMæˆ–å¸Œæœ›é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨è®­ç»ƒï¼Œæ‚¨å¯ä»¥ç›´æ¥æå–`scripts/train-pt.sh`ä¸­çš„`torchrun`å¼€å§‹è®­ç»ƒã€‚

</details>

<details>
<summary>

### å¾®è°ƒ

</summary>

#### æ•°æ®é›†å‡†å¤‡

å¾®è°ƒéœ€è¦çš„æ•°æ®é›†æ ¼å¼ä¸é¢„è®­ç»ƒç±»ä¼¼ï¼Œæ•°æ®é›†æ–‡ä»¶éœ€è¦ä¸ºjsonlæ ¼å¼ï¼šæ¯è¡Œä¸€ä¸ªjsonï¼Œå…¶ä¸­éœ€è¦åŒ…å«`"text"`å­—æ®µï¼Œå°†instructionã€inputå’Œoutputå…¨éƒ¨æŒ‰ç…§æ‚¨éœ€è¦çš„æ¨¡æ¿è¿›è¡Œæ‹¼æ¥ã€‚

ç„¶åéœ€è¦å°†æ•°æ®é›†åç§°å’Œè·¯å¾„æ³¨å†Œåˆ°`data/datasets.toml`ä¸­ï¼š

```toml
[ShareGPT-Chinese]              # æ•°æ®é›†åç§°
splits = ["train"]              # æ•°æ®é›†train/validé›†åˆ
root = "{DATA_DIR}/sft/{name}"  # æ•°æ®é›†æ ¹ç›®å½•
doc = "{name}-{split}"          # æ•°æ®é›†æ–‡ä»¶å
```

#### å¼€å§‹è®­ç»ƒ

è®­ç»ƒå¯åŠ¨è„šæœ¬ä¸º`scripts/train-sft.sh`ã€‚å¯ä»¥é€šè¿‡ä¿®æ”¹å…¶ä¸­çš„`TRAIN_DATASETS`ä¿®æ”¹è®­ç»ƒæ•°æ®é›†å’Œæ•°æ®é›†æ¯”ä¾‹ï¼š

```shell
TRAIN_DATASETS=(
    1.0:ShareGPT-Chinese  # ä½¿ç”¨å…¨é‡ShareGPT-Chinese
    0.5:ShareGPT-English  # ä½¿ç”¨ShareGPT-Englishçš„50%æ•°æ®
)
```

å¦‚æœæ‚¨ä½¿ç”¨SLURMé›†ç¾¤ç®¡ç†ç³»ç»Ÿï¼Œå¯ä»¥é€šè¿‡`sbatch`è¿›è¡Œæäº¤ï¼š

```shell
$ sbatch scripts/train-sft.sh
```

å¦‚æœæ²¡æœ‰SLURMæˆ–å¸Œæœ›é€šè¿‡å‘½ä»¤è¡Œå¯åŠ¨è®­ç»ƒï¼Œæ‚¨å¯ä»¥ç›´æ¥æå–`scripts/train-sft.sh`ä¸­çš„`torchrun`å¼€å§‹è®­ç»ƒã€‚

</details>

## âœ’ï¸ å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰æ‰€å¸®åŠ©æˆ–ä½¿ç”¨äº†æœ¬é¡¹ç›®çš„ä»£ç ï¼Œè¯·å¼•ç”¨æœ¬é¡¹ç›®ï¼š

```bibtex
@misc{Chinese-Mixtral-8x7B,
    author = {HIT-SCIR},
    title = {Chinese-Mixtral-8x7B: An Open-Source Mixture-of-Experts LLM},
    year = {2024},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/HIT-SCIR/Chinese-Mixtral-8x7B}}
}
```

## ğŸŒŸ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=HIT-SCIR/Chinese-Mixtral-8x7B&type=Date)](https://star-history.com/#HIT-SCIR/Chinese-Mixtral-8x7B&Date)
